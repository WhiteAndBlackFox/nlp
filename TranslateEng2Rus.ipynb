{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TranslateEng2Rus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMGd6EP1QsY6xmofxwj1Ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteAndBlackFox/nlp/blob/lessons3/TranslateEng2Rus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCepaxyxEcQC"
      },
      "source": [
        "# Перевод текста на с испольванием эмбендинга word2vec fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzqIKPgsCF13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be25a1e7-7a94-4772-be2e-b493f2705f10"
      },
      "source": [
        "!pip install -q --upgrade nltk gensim bokeh pandas\n",
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 65.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 18.5 MB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 43.4 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=1208d394f5f011f093b60545be74081d0f2583a9ccd09e320eeb198daff9a318\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDvzXQcxbTft",
        "outputId": "af73e420-2e59-4bd3-ba38-5a6de6d9fbfb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from string import punctuation\n",
        "import wget\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eVR_s0TEdiz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a15ec57-b134-4a1f-9dc7-7f56fdee643d"
      },
      "source": [
        "wget.download('https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ru.0-5000.txt', 'en_ru.train.txt')\n",
        "wget.download('https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ru.5000-6500.txt', 'en_ru.test.txt')\n",
        "\n",
        "wget.download('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', 'cc.en.300.vec.gz')\n",
        "wget.download('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz', 'cc.ru.300.vec.gz')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cc.ru.300.vec.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar3aR-K1jMVF"
      },
      "source": [
        "ru_vec = KeyedVectors.load_word2vec_format(\"/content/cc.ru.300.vec.gz\")\n",
        "en_vec = KeyedVectors.load_word2vec_format(\"/content/cc.en.300.vec.gz\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNalGabAmWs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061fb835-17de-4130-c097-e822f270b348"
      },
      "source": [
        "ru_vec.most_similar([ru_vec['кот']])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('кот', 0.9999998807907104),\n",
              " ('котик', 0.7908911108970642),\n",
              " ('пес', 0.7660022377967834),\n",
              " ('Кот', 0.7640087008476257),\n",
              " ('котенок', 0.757697343826294),\n",
              " ('котяра', 0.7543642520904541),\n",
              " ('песик', 0.7248488664627075),\n",
              " ('пёс', 0.7238960266113281),\n",
              " ('котёнок', 0.7153405547142029),\n",
              " ('кошак', 0.7141522765159607)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePiKSQFXqaDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12cfb78-4238-44a1-cc72-d8be3685db3d"
      },
      "source": [
        "en_vec.most_similar([en_vec['cat']])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 1.0),\n",
              " ('cats', 0.8350197672843933),\n",
              " ('kitty', 0.8233399391174316),\n",
              " ('kitten', 0.8082815408706665),\n",
              " ('feline', 0.7533785104751587),\n",
              " ('moggie', 0.7111802101135254),\n",
              " ('cat.It', 0.7107158303260803),\n",
              " ('dog', 0.7078642249107361),\n",
              " ('cat.The', 0.7065413594245911),\n",
              " ('cat.I', 0.7062530517578125)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qulXC24kqidd"
      },
      "source": [
        "def load_word_pairs(filename):\n",
        "    en_ru_pairs = []\n",
        "    en_vectors = []\n",
        "    ru_vectors = []\n",
        "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
        "        for line in inpf:\n",
        "            en, ru = line.rstrip().split(\" \")\n",
        "            if en not in en_vec or ru not in ru_vec:\n",
        "                continue\n",
        "            en_ru_pairs.append((en, ru))\n",
        "            en_vectors.append(en_vec[en])\n",
        "            ru_vectors.append(ru_vec[ru])\n",
        "    return en_ru_pairs, np.array(en_vectors), np.array(ru_vectors)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guiYrgMQquTd"
      },
      "source": [
        "en_ru_train, X_train, Y_train = load_word_pairs(\"en_ru.train.txt\")\n",
        "en_ru_test, X_test, Y_test = load_word_pairs(\"en_ru.test.txt\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOd4ISKkrRsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94d5961-fa54-4c82-8e8d-2a50c75a9ab1"
      },
      "source": [
        "mapping = LinearRegression(fit_intercept=False)\n",
        "mapping.fit(X_train, Y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(fit_intercept=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGh_0yJ5r7Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764fa983-0a2b-4327-eec2-ae324d75a1f9"
      },
      "source": [
        "august = mapping.predict(en_vec[\"august\"].reshape(1, -1))\n",
        "ru_vec.most_similar(august)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апреля', 0.6513441801071167),\n",
              " ('марта', 0.6481823921203613),\n",
              " ('июля', 0.6434776186943054),\n",
              " ('ноября', 0.642888605594635),\n",
              " ('июня', 0.6415560245513916),\n",
              " ('октябрь', 0.6412094235420227),\n",
              " ('октября', 0.6398786306381226),\n",
              " ('сентября', 0.6378851532936096),\n",
              " ('августа', 0.6375009417533875),\n",
              " ('сентябрь', 0.637001097202301)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HJvSnSsT01"
      },
      "source": [
        "def precision(pairs, mapped_vectors, emb, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    num_matches = 0\n",
        "    for i, (_, dst) in enumerate(pairs):\n",
        "        similar = emb.most_similar([mapped_vectors[i]],topn=topn)\n",
        "        for word, p in similar:\n",
        "            if dst == word:\n",
        "                num_matches += 1\n",
        "    precision_val = num_matches / len(pairs)\n",
        "    return precision_val"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2hiNggdyXzd"
      },
      "source": [
        "def learn_transform(X, Y):\n",
        "    U, s, V = np.linalg.svd(np.matmul(X_train.T, Y_train))\n",
        "    W = np.matmul(U, V)\n",
        "    return W"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlKL-BvPQSu"
      },
      "source": [
        "W = learn_transform(X_train, Y_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSU1mxNPS2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af57935a-7cbf-4434-a501-25f1cc489dc2"
      },
      "source": [
        "print(precision(en_ru_test, np.matmul(X_test, W), ru_vec))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2865291794646775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2EANHU0yR1"
      },
      "source": [
        "with open(\"/content/gdrive/MyDrive/Colab Notebooks/nlp/Lessons 3 - Embedding word2vec fasttext/fairy_tales.txt\", \"r\") as f:\n",
        "    en_sentences = [line.rstrip().lower() for line in f]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTt-SVZTOADu"
      },
      "source": [
        "def translate(sentence):\n",
        "    words = sentence.split(\" \")\n",
        "    translation = []\n",
        "    for word in words:\n",
        "        # try:\n",
        "            emb = en_vec[word]\n",
        "            translation.append(ru_vec.most_similar([np.matmul(emb, W)], topn=1)[0][0])           \n",
        "        # except:\n",
        "        #     translation.append(word)\n",
        "    return \" \".join(translation)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxj8nUFSORze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5224a5da-5e88-47f8-edde-75ef0771cd50"
      },
      "source": [
        "# Ну, почти нормльно xD\n",
        "translate(\"Yes i am work\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Да ,,я я работу'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}